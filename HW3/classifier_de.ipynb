{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1ZwMcgBZR-b59IRcvY2ww4hGvz6DfSGVY","timestamp":1706465998434}],"authorship_tag":"ABX9TyOdGluyfR38IIsvu/unP58p"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# python librarires for training dataset\n","- numpy (np): For numerical operations and array handling.\n","- tensorflow (tf): For building and training neural network models.\n","- train_test_split: For splitting datasets into training and - testing sets.\n","- Tokenizer: For converting text data into numerical sequences.\n","- pad_sequences: For padding sequences to ensure uniform length.\n","- Counter: For counting element occurrences in lists."],"metadata":{"id":"bU0P694u0o2-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"1A9Zx2zLsK8z"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from collections import Counter"]},{"cell_type":"markdown","source":["# get dataset from txt. file\n","- load_sentences function: This function takes a file path as input, reads the content of the file, and stores each line (stripped of leading and trailing whitespaces) as an element in a list. It then returns the list of sentences.\n","- data: The code loads sentences from a file named 'data_de.txt' using the defined function and stores them in a NumPy array."],"metadata":{"id":"C6HZmzvN1Btj"}},{"cell_type":"code","source":["# function to load sentences from a text file\n","def load_sentences(file_path):\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        sentences = [line.strip() for line in file]\n","    return sentences\n","\n","# load data to array\n","data = load_sentences('data_de.txt')\n","data = np.array(data)\n","\n","# print the first 10 sentences\n","print(\"Example Sentences:\")\n","print(data[:5])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5mGjVRKessB5","executionInfo":{"status":"ok","timestamp":1706521793161,"user_tz":-180,"elapsed":445,"user":{"displayName":"Enes Aysu","userId":"11025106623316829973"}},"outputId":"d750c7f9-dd0e-43a1-8aac-8930e3c9378b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Example Sentences:\n","[\"Lig ekiplerinden Çanakkale Dardanel' de oynamaktadır\"\n"," 'Her iki yöntem de X' 'Viyana Üniversitesi’n de hukuk eğitimine başladı'\n"," 'Warner Brothers, Columbia, Capitol şirketleriyle de çalışmıştır'\n"," 'Beş kıta da yayın yapmaktadır']\n"]}]},{"cell_type":"markdown","source":["# generate labels for the sentences based on their position\n","- Labels generation: It creates labels for the sentences using list comprehension. Sentences with even indices are labeled as 1 (suffix \"de's\"), and those with odd indices are labeled as 0 (conjunction \"de's\").\n","- Conversion to NumPy array: It converts the generated labels list into a NumPy array for further processing."],"metadata":{"id":"DUxlSzej1bDu"}},{"cell_type":"code","source":["# label the sentences (suffix de's are labeled as 1, conjunction de's are labeled as 0)\n","labels = [1 if i % 2 == 0 else 0 for i in range(20000)]\n","labels = np.array(labels)"],"metadata":{"id":"HdQWmm9ws7Gp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"cHuSrS4l1lW2"}},{"cell_type":"markdown","source":["# count the occurrences of unique words\n","- counter_word function: This function takes a collection of text data (text_col) as input. It initializes a Counter object to store word counts. Then, it iterates over each text in the collection, splits it into words, and updates the count for each word in the Counter object. Finally, it returns the Counter object containing the word counts.\n","- counter: The function is called with the data array (presumably containing sentences) to count the occurrences of unique words in the dataset."],"metadata":{"id":"tk9LPDyw1rK9"}},{"cell_type":"code","source":["# count unique words\n","def counter_word(text_col):\n","    count = Counter()\n","    for text in text_col:\n","        for word in text.split():\n","            count[word] += 1\n","    return count\n","\n","counter = counter_word(data)\n","print(len(counter))\n","counter.most_common(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9KshwFkOwykJ","executionInfo":{"status":"ok","timestamp":1706521812619,"user_tz":-180,"elapsed":555,"user":{"displayName":"Enes Aysu","userId":"11025106623316829973"}},"outputId":"453c8234-c55d-45e7-be59-ccbf17db23f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["34899\n"]},{"output_type":"execute_result","data":{"text/plain":["[('de', 10554), ('da', 9797), ('ve', 1336), ('olarak', 1124), ('vardır', 1049)]"]},"metadata":{},"execution_count":42}]},{"cell_type":"markdown","source":["# set parameters for preprocessing the data\n","- max_words: It determines the maximum number of words to tokenize. In this case, it appears to be set to the total number of unique words in the dataset, obtained from the previously calculated counter object.\n","- max_len: It sets the maximum length of sentences after tokenization. In this case, sentences longer than 7 words will be truncated, and shorter sentences will be padded with zeros to match this length."],"metadata":{"id":"CRSOt_AU1_7f"}},{"cell_type":"code","source":["# preprocess the data\n","max_words = len(counter)  # maximum number of words to tokenize\n","max_len = 7  # maximum length of sentences\n","\n","max_words"],"metadata":{"id":"mfzXY9u5tFGa","executionInfo":{"status":"ok","timestamp":1706521815140,"user_tz":-180,"elapsed":561,"user":{"displayName":"Enes Aysu","userId":"11025106623316829973"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6576bba4-ae53-471c-da63-0e046b08ace7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["34899"]},"metadata":{},"execution_count":43}]},{"cell_type":"markdown","source":["# tokenizes all words in the data and pads sequences\n","- Tokenizer initialization: The Tokenizer object is initialized with the num_words parameter set to max_words, which determines the maximum number of words to tokenize.\n","- Fitting tokenizer on data: The Tokenizer's fit_on_texts method is called with the data array to update the tokenizer's internal vocabulary based on the text data.\n","- Converting texts to sequences: The texts_to_sequences method of the tokenizer is used to convert each text in the data to a sequence of integers based on the tokenizer's vocabulary. Each word in the text is replaced by its corresponding integer index in the tokenizer's word index.\n","- Padding sequences: The pad_sequences function is used to ensure that all sequences have the same length (max_len). Sequences longer than max_len are truncated, and sequences shorter than max_len are padded with zeros at the beginning."],"metadata":{"id":"5ed7krxq2T0_"}},{"cell_type":"code","source":["# tokenize all words in the data\n","tokenizer = Tokenizer(num_words=max_words)\n","tokenizer.fit_on_texts(data)\n","sequences = tokenizer.texts_to_sequences(data)\n","padded_sequences = pad_sequences(sequences, maxlen=max_len)"],"metadata":{"id":"lQrdUNfZtMJD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# split the data into training and test sets\n","- Calculating split size: It calculates the index (train_size) where the split between training and test data should occur. This index corresponds to 80% of the total data length.\n","- Splitting sequences and labels: It splits the padded sequences (padded_sequences) and labels (labels) arrays into training and test sets using array slicing. The first train_size elements are assigned to the training set, while the remaining elements are assigned to the test set.\n","- Assignment: The training sequences and labels are assigned to train_sentences and train_labeled variables, respectively, while the test sequences and labels are assigned to test_sentences and test_labeled variables."],"metadata":{"id":"Z7eqeOyh2lea"}},{"cell_type":"code","source":["# split data into training and test sentences (%80 of data as training, %20 of data as test)\n","train_size = int(len(data) * 0.8)\n","train_sentences = padded_sequences[:train_size]\n","train_labeled = labels[:train_size]\n","test_sentences = padded_sequences[train_size:]\n","test_labeled = labels[train_size:]"],"metadata":{"id":"8Kjn-OrStRSZ","executionInfo":{"status":"ok","timestamp":1706521819691,"user_tz":-180,"elapsed":550,"user":{"displayName":"Enes Aysu","userId":"11025106623316829973"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"57fd5a5c-0590-4c73-9fac-40a55dfbc508"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[   0   48   49 1830 9086    1   68]\n"," [   0    0   42   40 1192    1  598]\n"," [   0 1339 2761    1  568 1055   44]\n"," [2230 9087 9088 5129 9089    1  521]\n"," [   0    0  368 3582    2  964  152]]\n","[1 0 1 0 1]\n"]}]},{"cell_type":"markdown","source":["# defines a neural network model using TensorFlow's Keras API\n","- Model architecture: It defines a sequential model using tf.keras.Sequential(), which allows for building models layer by layer. The model consists of the following layers:\n","    - Embedding layer: Converts integer-encoded words into dense vectors of fixed size (64 dimensions in this case). It expects sequences of integers as input and has a vocabulary size of max_words. The input_length parameter specifies the length of input sequences.\n","    - LSTM layer: Long Short-Term Memory (LSTM) layer with 64 units and ReLU activation function. LSTMs are a type of recurrent neural network (RNN) capable of learning long-term dependencies in sequence data.\n","    - Dense layer: Output layer with a single neuron and sigmoid activation function, suitable for binary classification tasks.\n","- Compilation: It compiles the model using the compile() method, specifying the loss function (binary_crossentropy for binary classification), optimizer (adam), and evaluation metric (accuracy)."],"metadata":{"id":"s6_zkwB427Rf"}},{"cell_type":"code","source":["# build model\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(max_words, 64, input_length=max_len),\n","    tf.keras.layers.LSTM(64, activation='relu'),\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n","\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"],"metadata":{"id":"jCqIUm6VtrRa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# train the defined model using the training data and evaluates its performance\n","- Training the model: The fit() method is called on the model with the following parameters:\n","    - train_sentences and train_labeled: The training sequences and their corresponding labels.\n","    - epochs=10: The number of training epochs, set to 10 in this case, meaning the entire training dataset will be passed forward and backward through the neural network 10 times during training.\n","    - validation_data=(test_sentences, test_labeled): The validation data to evaluate the model's performance after each epoch. It consists of the test sequences and their labels.\n","- History object: The fit() method returns a History object (history) containing information about the training process, such as loss and accuracy metrics recorded at each epoch."],"metadata":{"id":"gN1CGaRT3UeN"}},{"cell_type":"code","source":["history = model.fit(train_sentences, train_labeled, epochs=10, validation_data=(test_sentences, test_labeled))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vJTcNqLTt1BS","executionInfo":{"status":"ok","timestamp":1706522211473,"user_tz":-180,"elapsed":262567,"user":{"displayName":"Enes Aysu","userId":"11025106623316829973"}},"outputId":"05203805-bbb4-44d2-e988-caafb6e21fea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","500/500 [==============================] - 21s 41ms/step - loss: 0.0082 - accuracy: 0.9977 - val_loss: 0.5922 - val_accuracy: 0.8545\n","Epoch 2/10\n","500/500 [==============================] - 20s 40ms/step - loss: 0.0059 - accuracy: 0.9979 - val_loss: 0.8401 - val_accuracy: 0.8605\n","Epoch 3/10\n","500/500 [==============================] - 21s 42ms/step - loss: 0.0038 - accuracy: 0.9986 - val_loss: 0.9572 - val_accuracy: 0.8522\n","Epoch 4/10\n","500/500 [==============================] - 20s 41ms/step - loss: 0.0032 - accuracy: 0.9991 - val_loss: 1.0265 - val_accuracy: 0.8540\n","Epoch 5/10\n","500/500 [==============================] - 20s 41ms/step - loss: 0.0085 - accuracy: 0.9979 - val_loss: 0.6545 - val_accuracy: 0.8600\n","Epoch 6/10\n","500/500 [==============================] - 21s 41ms/step - loss: 0.0032 - accuracy: 0.9992 - val_loss: 0.9441 - val_accuracy: 0.8580\n","Epoch 7/10\n","500/500 [==============================] - 20s 39ms/step - loss: 0.0016 - accuracy: 0.9994 - val_loss: 1.1218 - val_accuracy: 0.8508\n","Epoch 8/10\n","500/500 [==============================] - 20s 41ms/step - loss: 0.0036 - accuracy: 0.9989 - val_loss: 1.0166 - val_accuracy: 0.8545\n","Epoch 9/10\n","500/500 [==============================] - 21s 42ms/step - loss: 0.0021 - accuracy: 0.9994 - val_loss: 1.1377 - val_accuracy: 0.8500\n","Epoch 10/10\n","500/500 [==============================] - 20s 39ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 1.3353 - val_accuracy: 0.8497\n"]}]},{"cell_type":"markdown","source":["# evaluate the trained model on the test data to calculate its loss and accuracy\n","- Model evaluation: The evaluate() method is called on the model with the test sequences (test_sentences) and their corresponding labels (test_labeled) as input.\n","- Test loss and accuracy: The method returns the test loss and accuracy, which are assigned to the variables test_loss and test_acc, respectively."],"metadata":{"id":"zXFAPnq13mny"}},{"cell_type":"code","source":["test_loss, test_acc = model.evaluate(test_sentences, test_labeled)\n","print(\"Test Accuracy:\", test_acc)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CpUEpm6QupQd","executionInfo":{"status":"ok","timestamp":1706522219890,"user_tz":-180,"elapsed":1081,"user":{"displayName":"Enes Aysu","userId":"11025106623316829973"}},"outputId":"03eecd6a-ee20-4e01-cc87-f2e3a27b535c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["125/125 [==============================] - 0s 3ms/step - loss: 1.3353 - accuracy: 0.8497\n","Test Accuracy: 0.8497499823570251\n"]}]},{"cell_type":"markdown","source":["# analyze sentences and making predictions using the trained model\n","- analyze_sentence function: This function takes a sentence as input, splits it into words, and analyzes each word to identify certain suffix patterns (\"da\", \"de\", \"ta\", \"te\"). If a word meets the criteria, it separates the suffix and root and appends them to a list. Finally, it joins the analyzed words back into a sentence and returns it.\n","- predict function: This function takes a sentence and an optional threshold (default is 0.5) as input. It tokenizes and pads the sentence, predicts the label and score using the trained model, and returns the prediction label along with the prediction score.\n","- Sentences for prediction: A list of sample sentences is provided for prediction.\n","- Prediction loop: Each sentence from the list is passed through the analyze_sentence function to preprocess it and then through the predict function for prediction. The prediction label and score are printed for each sentence."],"metadata":{"id":"6FXkTJc23w7C"}},{"cell_type":"code","source":["# make predictions\n","def analyze_sentence(sentence):\n","    words = sentence.split()\n","    analyzed_words = []\n","\n","    for word in words:\n","        if len(word) >= 3 and (word.endswith(\"da\") or word.endswith(\"de\") or word.endswith(\"ta\") or word.endswith(\"te\")):\n","            suffix = word[-2:]\n","            root = word[:-2]\n","            analyzed_words.append(root + \" \" + suffix)\n","        else:\n","            analyzed_words.append(word)\n","\n","    analyzed_sentence = \" \".join(analyzed_words)\n","    return analyzed_sentence\n","\n","def predict(sentence, threshold=0.5):\n","    sequence = tokenizer.texts_to_sequences([sentence])\n","    padded_sequence = pad_sequences(sequence, maxlen=max_len)\n","    prediction_score = model.predict(padded_sequence)[0][0]\n","    if prediction_score >= threshold:\n","        return \"'de/da' is suffix so this sentence is true written\", prediction_score\n","    else:\n","        return \"'de/da' is conjunction so this sentence is false written\", prediction_score\n","\n","sentences = [\"Karşıda bir adam bekliyor.\", \"Yolda giderken arkadaşı ile karşılaşmış.\", \"Bu mevsimlerde hava sert olur.\",\n","             \"Arabada giderken birden önüne kedi çıkmış.\", \"Beslenme çantasını okulda bırakmış.\", \"Ordularını Eskişehir'de bekletti.\",\n","             \"Küçüklüğünde müzik ile uğraşırdı.\", \"Bu olay kendisini beklediğindende fazla etkiledi.\", \"Gitsede kalsada fark etmez.\",\n","             \"İç anadolu iklimi bölgede hakimdir.\"]\n","\n","for sentence in sentences:\n","    prediction_label, prediction_score = predict(analyze_sentence(sentence))\n","    print(\"Prediction Label:\", prediction_label)\n","    print(\"Prediction Score:\", prediction_score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fIFKiuziurDM","executionInfo":{"status":"ok","timestamp":1706531374762,"user_tz":-180,"elapsed":1085,"user":{"displayName":"Enes Aysu","userId":"11025106623316829973"}},"outputId":"9cc0279b-b3b9-410d-fbb2-9cc713eda9a1"},"execution_count":88,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 21ms/step\n","Prediction Label: 'de/da' is conjunction so this sentence is false written\n","Prediction Score: 1.3576685e-19\n","1/1 [==============================] - 0s 22ms/step\n","Prediction Label: 'de/da' is suffix so this sentence is true written\n","Prediction Score: 0.99665934\n","1/1 [==============================] - 0s 22ms/step\n","Prediction Label: 'de/da' is suffix so this sentence is true written\n","Prediction Score: 0.99990034\n","1/1 [==============================] - 0s 22ms/step\n","Prediction Label: 'de/da' is suffix so this sentence is true written\n","Prediction Score: 0.98962843\n","1/1 [==============================] - 0s 22ms/step\n","Prediction Label: 'de/da' is conjunction so this sentence is false written\n","Prediction Score: 2.3659767e-11\n","1/1 [==============================] - 0s 22ms/step\n","Prediction Label: 'de/da' is conjunction so this sentence is false written\n","Prediction Score: 2.6123769e-06\n","1/1 [==============================] - 0s 21ms/step\n","Prediction Label: 'de/da' is conjunction so this sentence is false written\n","Prediction Score: 6.469985e-08\n","1/1 [==============================] - 0s 22ms/step\n","Prediction Label: 'de/da' is conjunction so this sentence is false written\n","Prediction Score: 3.19626e-05\n","1/1 [==============================] - 0s 22ms/step\n","Prediction Label: 'de/da' is conjunction so this sentence is false written\n","Prediction Score: 2.1169672e-08\n","1/1 [==============================] - 0s 21ms/step\n","Prediction Label: 'de/da' is suffix so this sentence is true written\n","Prediction Score: 1.0\n"]}]}]}